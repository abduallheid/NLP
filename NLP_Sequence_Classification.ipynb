{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Sequence-Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMs3KSLJRu1bWLDlFt1R7lR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abduallheid/NLP/blob/main/NLP_Sequence_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzwfPc3_SrQK"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence Classification"
      ],
      "metadata": {
        "id": "w47ajeJRX3Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "EQs7De4HSsTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This returns a label (\"POSITIVE\" or \"NEGATIVE\") alongside a score, as follows:"
      ],
      "metadata": {
        "id": "jw6OD0n8YEgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"We are very sad to do homework .\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52X463TYYUkq",
        "outputId": "95c9cac0-779e-4438-8452-92566fc21d10"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9982720613479614}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(\"We are very executed to show you now ðŸ¤— .\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQxTd0yeTPJ9",
        "outputId": "b2cfe348-309c-4dee-c75e-66e8303a7693"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9990133047103882}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Here is an example of doing a sequence classification using a model to determine if two sequences are paraphrases of each other"
      ],
      "metadata": {
        "id": "Jmb9wgBVZzEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Torch"
      ],
      "metadata": {
        "id": "z_y8EofPa40a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "\n",
        "sequence_0 = \"The company BMW is based in Munich City\"\n",
        "sequence_1 = \"Apples are especially bad for your health\"\n",
        "sequence_2 = \"BMW's headquarters are situated in Munich\"\n",
        "\n",
        "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
        "# the sequence, as well as compute the attention masks.\n",
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
        "\n",
        "paraphrase_classification_logits = model(**paraphrase).logits\n",
        "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
        "\n",
        "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "Ueqwcw8pYv5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Should be paraphrase\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MudiGHqjaicU",
        "outputId": "e2bc7d48-aadc-4464-e895-e94fba4d010c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 6%\n",
            "is paraphrase: 94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Should not be paraphrase\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxRp6EuSalqu",
        "outputId": "f8cd0cc4-789b-49c3-e57b-112096c25623"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using tensorflow "
      ],
      "metadata": {
        "id": "tTSFRSp_bDWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "\n",
        "sequence_0 = \"The company BMW is based in Munich City\"\n",
        "sequence_1 = \"Apples are especially bad for your health\"\n",
        "sequence_2 = \"BMW's headquarters are situated in Munich\"\n",
        "\n",
        "# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to\n",
        "# the sequence, as well as compute the attention masks.\n",
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"tf\")\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"tf\")\n",
        "\n",
        "paraphrase_classification_logits = model(paraphrase).logits\n",
        "not_paraphrase_classification_logits = model(not_paraphrase).logits\n",
        "\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\n",
        "not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]\n"
      ],
      "metadata": {
        "id": "pLli28cFbOhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Should be paraphrase\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUuz0yUZbvug",
        "outputId": "89be1b96-d5cf-4b74-c9fc-a5ede35114c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 6%\n",
            "is paraphrase: 94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Should not be paraphrase\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UHw4BIib1Z5",
        "outputId": "64169d4f-9a86-4ceb-dda3-559aad736e6a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n"
          ]
        }
      ]
    }
  ]
}